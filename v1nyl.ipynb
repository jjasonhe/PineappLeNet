{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v1nyl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noodles/.conda/envs/turntable/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/noodles/.conda/envs/turntable/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import h5py as h5\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_step(slices):\n",
    "\tnum_slice,_,_,_ = slices.shape\n",
    "\tprint(num_slice)\n",
    "\tslices = slices.transpose(0,2,1,3)\n",
    "#\tslices = tf.transpose(slices, perm=[0,2,1,3])\n",
    "#\tprint(slices.dtype)\n",
    "\tfor s in range(num_slice):\n",
    "\t\tfig = plt.figure(s)\n",
    "\t\t# a = plt.subplot(num_slice, 1, 1+s)\n",
    "\t\tplt.imshow(np.abs(slices[s,:,:,0]), cmap='gray')\n",
    "#\t\tprint(slices[s,:,:,0].dtype.base_dtype)\n",
    "# \t\tplt.imshow(slices[s,:,:,0], cmap='gray')\n",
    "#\t\tplt.imshow(slices[s,:,:,0])\n",
    "\t\tplt.title(\"%d\" % s)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train, num_val, num_test = 70, 10, 20\n",
    "\n",
    "train_data,val_data,test_data = du.create_dicts(num_train, num_val, num_test)\n",
    "maxX,maxY,maxT = du.init() # call once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creates h5 files of the data, each patient is a dataset\n",
    "# du.train_h5(train_data, maxX, maxY, maxT)\n",
    "# du.val_h5(val_data, maxX, maxY, maxT)\n",
    "# du.test_h5(test_data, maxX, maxY, maxT)\n",
    "\n",
    "# Creates tfrecords files where data is sorted slice-by-slice, in chunks of (T,H,W,C)\n",
    "# du.what_tf('train', train_data)\n",
    "# du.what_tf('val', val_data)\n",
    "# du.what_tf('test', test_data)\n",
    "\n",
    "# Creates tfrecords files where data is sorted image-by-image, in chunks of (H,W,C)\n",
    "# du.what_tf_fcn8('train', train_data)\n",
    "# du.what_tf_fcn8('val', val_data)\n",
    "# du.what_tf_fcn8('test', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fc = h5.File('datasets/train_curr.h5', 'r')\n",
    "# fn = h5.File('datasets/train_next.h5', 'r')\n",
    "# print(fc[train_data[0]].shape)\n",
    "# print(fn[train_data[0]].shape)\n",
    "# plot_time_step(fc[train_data[0]][510], fn[train_data[0]][510])\n",
    "f = h5.File('datasets/train.h5', 'r')\n",
    "print(f[train_data[0]].shape)\n",
    "print(f[train_data[0]].dtype)\n",
    "plot_time_step(f[train_data[0]][25,1:18,:,:,:])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model-001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
    "  \"\"\"A LSTM cell with convolutions instead of multiplications.\n",
    "  Reference:\n",
    "    Xingjian, S. H. I., et al. \"Convolutional LSTM network: A machine learning approach for precipitation nowcasting.\" Advances in Neural Information Processing Systems. 2015.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, shape, filters, kernel, forget_bias=1.0, activation=tf.tanh, normalize=True, peephole=True, data_format='channels_last', reuse=None):\n",
    "    super(ConvLSTMCell, self).__init__(_reuse=reuse)\n",
    "    self._kernel = kernel\n",
    "    self._filters = filters\n",
    "    self._forget_bias = forget_bias\n",
    "    self._activation = activation\n",
    "    self._normalize = normalize\n",
    "    self._peephole = peephole\n",
    "    if data_format == 'channels_last':\n",
    "        self._size = tf.TensorShape(shape + [self._filters])\n",
    "        self._feature_axis = self._size.ndims\n",
    "        self._data_format = None\n",
    "    elif data_format == 'channels_first':\n",
    "        self._size = tf.TensorShape([self._filters] + shape)\n",
    "        self._feature_axis = 0\n",
    "        self._data_format = 'NC'\n",
    "    else:\n",
    "        raise ValueError('Unknown data_format')\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return tf.nn.rnn_cell.LSTMStateTuple(self._size, self._size)\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._size\n",
    "\n",
    "  def call(self, x, state):\n",
    "    c, h = state\n",
    "\n",
    "    x = tf.concat([x, h], axis=self._feature_axis)\n",
    "    n = x.shape[-1].value\n",
    "    m = 4 * self._filters if self._filters > 1 else 4\n",
    "    W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "    y = tf.nn.convolution(x, W, 'SAME', data_format=self._data_format)\n",
    "    if not self._normalize:\n",
    "      y += tf.get_variable('bias', [m], initializer=tf.zeros_initializer())\n",
    "    j, i, f, o = tf.split(y, 4, axis=self._feature_axis)\n",
    "\n",
    "    if self._peephole:\n",
    "      i += tf.get_variable('W_ci', c.shape[1:]) * c\n",
    "      f += tf.get_variable('W_cf', c.shape[1:]) * c\n",
    "\n",
    "    if self._normalize:\n",
    "      j = tf.contrib.layers.layer_norm(j)\n",
    "      i = tf.contrib.layers.layer_norm(i)\n",
    "      f = tf.contrib.layers.layer_norm(f)\n",
    "\n",
    "    f = tf.sigmoid(f + self._forget_bias)\n",
    "    i = tf.sigmoid(i)\n",
    "    c = c * f + i * self._activation(j)\n",
    "\n",
    "    if self._peephole:\n",
    "      o += tf.get_variable('W_co', c.shape[1:]) * c\n",
    "\n",
    "    if self._normalize:\n",
    "      o = tf.contrib.layers.layer_norm(o)\n",
    "      c = tf.contrib.layers.layer_norm(c)\n",
    "\n",
    "    o = tf.sigmoid(o)\n",
    "    h = o * self._activation(c)\n",
    "\n",
    "    state = tf.nn.rnn_cell.LSTMStateTuple(c, h)\n",
    "\n",
    "    return h, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial ConvLSTM-based model\n",
    "\n",
    "def model_fn(features,\n",
    "             labels,\n",
    "             mode,\n",
    "             params):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "    \n",
    "        img_curr = features\n",
    "        img_next = labels\n",
    "    \n",
    "        inputs = img_curr\n",
    "    \n",
    "        #tf.summary.image('input', inputs[0,:,:,:,:], max_outputs=200)\n",
    "    \n",
    "        #print(inputs.get_shape())\n",
    "    \n",
    "        #inputs = tf.split(value=img_curr, num_or_size_splits=17)\n",
    "        #print('inputs={}'.format(inputs))\n",
    "    \n",
    "        convlstm_cell = rnn.Conv2DLSTMCell(\n",
    "            input_shape=[224, 192, 1],\n",
    "            output_channels=1,\n",
    "            kernel_shape=[5,5],\n",
    "            use_bias=True,\n",
    "            skip_connection=False,\n",
    "            forget_bias=params.forget_bias\n",
    "        )\n",
    "    \n",
    "        outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell=convlstm_cell,\n",
    "            inputs=inputs,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    \n",
    "        print('outputs={}'.format(outputs))\n",
    "    \n",
    "        # Adds output predictions to be displayed in TensorBoard\n",
    "        #tf.summary.image('output', outputs[0,:,:,:,:], max_outputs=200)\n",
    "    \n",
    "        disp = tf.concat([inputs[0,:,:,:,:], outputs[0,:,:,:,:]], 1)\n",
    "        tf.summary.image('concat', disp, max_outputs=200)\n",
    "    \n",
    "        predictions = {'results': outputs}\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            export_outputs = {\n",
    "                'predictions': tf.estimator.export.PredictOutput(outputs)\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=outputs\n",
    "            )\n",
    "    \n",
    "        loss = tf.losses.mean_squared_error(img_next, outputs)\n",
    "    \n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=params.learning_rate,\n",
    "            beta1=params.beta1,\n",
    "            beta2=params.beta2,\n",
    "            epsilon=params.epsilon\n",
    "        )\n",
    "    \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "        eval_metric_ops = {\n",
    "            \"rmse\": tf.metrics.root_mean_squared_error(img_next, outputs),\n",
    "            \"mae\": tf.metrics.mean_absolute_error(img_next, outputs),\n",
    "            \"mse\": tf.metrics.mean_squared_error(img_next, outputs)\n",
    "        }\n",
    "    \n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=eval_metric_ops\n",
    "        )\n",
    "        \n",
    "        return estimator_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model function based on FCN8\n",
    "\n",
    "def model_fn(features,\n",
    "             labels,\n",
    "             mode,\n",
    "             params):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        inputs = features\n",
    "        \n",
    "        conv_out = tf.layers.conv2d(inputs, filters=32, kernel_size=5, strides=1, padding='same')\n",
    "        \n",
    "        outputs = tf.layers.conv2d_transpose(conv_out, filters=1, kernel_size=5, strides=1, padding='same')\n",
    "    \n",
    "        # Adds output predictions to be displayed in TensorBoard\n",
    "        #tf.summary.image('output', outputs[0,:,:,:,:], max_outputs=200)\n",
    "        \n",
    "        # Adds concatenated inputs and outputs to be displayed in TensorBoard\n",
    "        disp = tf.concat([inputs[:,:,:], outputs[:,:,:]], 1)\n",
    "        tf.summary.image('concat', disp, max_outputs=200)\n",
    "    \n",
    "        predictions = {'results': outputs}\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            export_outputs = {\n",
    "                'predictions': tf.estimator.export.PredictOutput(outputs)\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=outputs\n",
    "            )\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            disp = tf.concat([inputs[:,:,:], outputs[:,:,:]], 1)\n",
    "            tf.summary.image('concat', disp, max_outputs=200)\n",
    "    \n",
    "        loss = tf.losses.mean_squared_error(labels, outputs)\n",
    "    \n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=params.learning_rate,\n",
    "            beta1=params.beta1,\n",
    "            beta2=params.beta2,\n",
    "            epsilon=params.epsilon\n",
    "        )\n",
    "    \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "        eval_metric_ops = {\n",
    "            \"rmse\": tf.metrics.root_mean_squared_error(labels, outputs),\n",
    "            \"mae\": tf.metrics.mean_absolute_error(labels, outputs),\n",
    "            \"mse\": tf.metrics.mean_squared_error(labels, outputs)\n",
    "        }\n",
    "    \n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=eval_metric_ops\n",
    "        )\n",
    "        \n",
    "        return estimator_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_train_batch(i):\n",
    "    print(\"Preparing training dataset {}\".format(i))\n",
    "    ftr = h5.File('datasets/train.h5', 'r')\n",
    "    img = ftr[train_data[i]]\n",
    "    img_curr = np.absolute(img[:,:-1]).astype(np.float32)\n",
    "    max_curr = np.amax(img_curr, axis=(1,2,3))\n",
    "    img_curr = img_curr/max_curr[:,np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    img_next = np.absolute(img[:,1:]).astype(np.float32)\n",
    "    max_next = np.amax(img_next, axis=(1,2,3))\n",
    "    img_next = img_next/max_next[:,np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_curr, img_next))\n",
    "    dataset = dataset.batch(1)\n",
    "    return dataset\n",
    "\n",
    "def fetch_val_batch(i):\n",
    "    print(\"Preparing validation dataset {}\".format(i))\n",
    "    ftr = h5.File('datasets/val.h5', 'r')\n",
    "    img = ftr[val_data[i]]\n",
    "    img_curr = np.absolute(img[:,:-1]).astype(np.float32)\n",
    "    max_curr = np.amax(img_curr, axis=(1,2,3))\n",
    "    img_curr = img_curr/max_curr[:,np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    img_next = np.absolute(img[:,1:]).astype(np.float32)\n",
    "    max_next = np.amax(img_next, axis=(1,2,3))\n",
    "    img_next = img_next/max_next[:,np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_curr, img_next))\n",
    "    dataset = dataset.batch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        num_epochs=1,\n",
    "        #batch_size = 80,\n",
    "        forget_bias=1.0,\n",
    "        learning_rate=0.001,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        epsilon=1\n",
    "    )\n",
    "    \n",
    "    session_config = tf.ConfigProto()\n",
    "    session_config.gpu_options.allow_growth = True\n",
    "    session_config.allow_soft_placement = True\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        log_step_count_steps=10,\n",
    "        save_summary_steps=10,\n",
    "        tf_random_seed=19830610,\n",
    "        model_dir=os.path.join('summary', 'train'),\n",
    "        session_config=session_config\n",
    "    )\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn, params=hparams, config=run_config)\n",
    "    \n",
    "#     train_dataset = du.create_dataset('train', train_data)\n",
    "#     train_dataset = du.create_dataset_fcn8('train', train_data)\n",
    "#     train_dataset = train_dataset.shuffle(1000)\n",
    "#     train_dataset = train_dataset.batch(35)\n",
    "    \n",
    "    val_dataset = du.create_dataset_fcn8('val', val_data)\n",
    "    val_dataset = val_dataset.batch(35)\n",
    "\n",
    "#     print(train_dataset.output_types)\n",
    "#     print(train_dataset.output_shapes)\n",
    "#     print(val_dataset.output_types)\n",
    "#     print(val_dataset.output_shapes)\n",
    "    \n",
    "#     def train_input_fn():\n",
    "#         train_iterator = train_dataset.make_one_shot_iterator()\n",
    "#         features, labels = train_iterator.get_next()\n",
    "#         return features, labels\n",
    "    \n",
    "    def val_input_fn():\n",
    "        val_iterator = val_dataset.make_one_shot_iterator()\n",
    "        features, labels = val_iterator.get_next()\n",
    "        return features, labels\n",
    "    \n",
    "#     estimator.train(input_fn=train_input_fn, max_steps=None)\n",
    "    estimator.evaluate(input_fn=val_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'summary/train', '_tf_random_seed': 19830610, '_save_summary_steps': 10, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1b6928f7b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-30-08:03:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from summary/train/model.ckpt-2605\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-30-08:04:16\n",
      "INFO:tensorflow:Saving dict for global step 2605: global_step = 2605, loss = 0.0013834072, mae = 0.01974854, mse = 0.00138199, rmse = 0.037175126\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noodles/.conda/envs/turntable/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
