{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tea5poon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noodles/.conda/envs/turntable/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/noodles/.conda/envs/turntable/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as wtf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 32\n",
    "\n",
    "num_train, num_val, num_test = 70, 10, 20\n",
    "train_data,val_data,test_data = du.create_dicts(num_train, num_val, num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient 0\n",
      "example 17\n",
      "patient 1\n",
      "example 37\n",
      "patient 2\n",
      "example 57\n",
      "patient 3\n",
      "example 75\n",
      "patient 4\n",
      "example 95\n",
      "patient 5\n",
      "example 115\n",
      "patient 6\n",
      "example 135\n",
      "patient 7\n",
      "example 153\n",
      "patient 8\n",
      "example 165\n",
      "patient 9\n",
      "example 187\n",
      "patient 0\n",
      "example 20\n",
      "patient 1\n",
      "example 38\n",
      "patient 2\n",
      "example 58\n",
      "patient 3\n",
      "example 80\n",
      "patient 4\n",
      "example 92\n",
      "patient 5\n",
      "example 114\n",
      "patient 6\n",
      "example 126\n",
      "patient 7\n",
      "example 138\n",
      "patient 8\n",
      "example 158\n",
      "patient 9\n",
      "example 178\n",
      "patient 10\n",
      "example 195\n",
      "patient 11\n",
      "example 212\n",
      "patient 12\n",
      "example 232\n",
      "patient 13\n",
      "example 252\n",
      "patient 14\n",
      "example 270\n",
      "patient 15\n",
      "example 290\n",
      "patient 16\n",
      "example 304\n",
      "patient 17\n",
      "example 318\n",
      "patient 18\n",
      "example 338\n",
      "patient 19\n",
      "example 358\n",
      "patient 0\n",
      "example 20\n",
      "patient 1\n",
      "example 40\n",
      "patient 2\n",
      "example 60\n",
      "patient 3\n",
      "example 81\n",
      "patient 4\n",
      "example 99\n",
      "patient 5\n",
      "example 119\n",
      "patient 6\n",
      "example 139\n",
      "patient 7\n",
      "example 159\n",
      "patient 8\n",
      "example 179\n",
      "patient 9\n",
      "example 199\n",
      "patient 10\n",
      "example 219\n",
      "patient 11\n",
      "example 238\n",
      "patient 12\n",
      "example 259\n",
      "patient 13\n",
      "example 278\n",
      "patient 14\n",
      "example 298\n",
      "patient 15\n",
      "example 318\n",
      "patient 16\n",
      "example 338\n",
      "patient 17\n",
      "example 358\n",
      "patient 18\n",
      "example 378\n",
      "patient 19\n",
      "example 398\n",
      "patient 20\n",
      "example 416\n",
      "patient 21\n",
      "example 431\n",
      "patient 22\n",
      "example 451\n",
      "patient 23\n",
      "example 471\n",
      "patient 24\n",
      "example 491\n",
      "patient 25\n",
      "example 507\n",
      "patient 26\n",
      "example 523\n",
      "patient 27\n",
      "example 539\n",
      "patient 28\n",
      "example 559\n",
      "patient 29\n",
      "example 579\n",
      "patient 30\n",
      "example 598\n",
      "patient 31\n",
      "example 618\n",
      "patient 32\n",
      "example 636\n",
      "patient 33\n",
      "example 656\n",
      "patient 34\n",
      "example 676\n",
      "patient 35\n",
      "example 695\n",
      "patient 36\n",
      "example 715\n",
      "patient 37\n",
      "example 733\n",
      "patient 38\n",
      "example 753\n",
      "patient 39\n",
      "example 770\n",
      "patient 40\n",
      "example 797\n",
      "patient 41\n",
      "example 815\n",
      "patient 42\n",
      "example 835\n",
      "patient 43\n",
      "example 854\n",
      "patient 44\n",
      "example 873\n",
      "patient 45\n",
      "example 893\n",
      "patient 46\n",
      "example 913\n",
      "patient 47\n",
      "example 933\n",
      "patient 48\n",
      "example 953\n",
      "patient 49\n",
      "example 973\n",
      "patient 50\n",
      "example 993\n",
      "patient 51\n",
      "example 1013\n",
      "patient 52\n",
      "example 1031\n",
      "patient 53\n",
      "example 1049\n",
      "patient 54\n",
      "example 1069\n",
      "patient 55\n",
      "example 1089\n",
      "patient 56\n",
      "example 1101\n",
      "patient 57\n",
      "example 1113\n",
      "patient 58\n",
      "example 1131\n",
      "patient 59\n",
      "example 1152\n",
      "patient 60\n",
      "example 1172\n",
      "patient 61\n",
      "example 1188\n",
      "patient 62\n",
      "example 1208\n",
      "patient 63\n",
      "example 1228\n",
      "patient 64\n",
      "example 1246\n",
      "patient 65\n",
      "example 1266\n",
      "patient 66\n",
      "example 1286\n",
      "patient 67\n",
      "example 1306\n",
      "patient 68\n",
      "example 1326\n",
      "patient 69\n",
      "example 1342\n"
     ]
    }
   ],
   "source": [
    "du.what_tf_g7_adaproon('val', val_data)\n",
    "du.what_tf_g7_adaproon('test', test_data)\n",
    "du.what_tf_g7_adaproon('train', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'tea5poon'\n",
    "OVERFIT = False\n",
    "\n",
    "MODEL_NAME = MODEL_TYPE + '-000'\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_fn(inputs):\n",
    "    conv1_1 = wtf.layers.conv2d(inputs,\n",
    "                                filters=64,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv1_1')\n",
    "    conv1_2 = wtf.layers.conv2d(conv1_1,\n",
    "                                filters=64,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv1_2')\n",
    "    pool1 = wtf.layers.max_pooling2d(conv1_2,\n",
    "                                     pool_size=2,\n",
    "                                     strides=2,\n",
    "                                     padding='same',\n",
    "                                     name='pool1')\n",
    "        \n",
    "    conv2_1 = wtf.layers.conv2d(pool1,\n",
    "                                filters=128,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv2_1')\n",
    "    conv2_2 = wtf.layers.conv2d(conv2_1,\n",
    "                                filters=128,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv2_2')\n",
    "    pool2 = wtf.layers.max_pooling2d(conv2_2,\n",
    "                                     pool_size=2,\n",
    "                                     strides=2,\n",
    "                                     padding='same',\n",
    "                                     name='pool2')\n",
    "        \n",
    "    conv3_1 = wtf.layers.conv2d(pool2,\n",
    "                                filters=256,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv3_1')\n",
    "    conv3_2 = wtf.layers.conv2d(conv3_1,\n",
    "                                filters=256,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv3_2')\n",
    "    conv3_3 = wtf.layers.conv2d(conv3_2,\n",
    "                                filters=256,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv3_3')\n",
    "    pool3 = wtf.layers.max_pooling2d(conv3_3,\n",
    "                                     pool_size=2,\n",
    "                                     strides=2,\n",
    "                                     padding='same',\n",
    "                                     name='pool3')\n",
    "        \n",
    "    conv4_1 = wtf.layers.conv2d(pool3,\n",
    "                                filters=512,\n",
    "                                kernel_size=1,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv4_1')\n",
    "    conv4_2 = wtf.layers.conv2d(conv4_1,\n",
    "                                filters=512,\n",
    "                                kernel_size=1,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv4_2')\n",
    "    conv4_3 = wtf.layers.conv2d(conv4_2,\n",
    "                                filters=512,\n",
    "                                kernel_size=1,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv4_3')\n",
    "    pool4 = wtf.layers.max_pooling2d(conv4_3,\n",
    "                                     pool_size=2,\n",
    "                                     strides=2,\n",
    "                                     padding='same',\n",
    "                                     name='pool4')\n",
    "    \n",
    "    conv5_1 = wtf.layers.conv2d(pool4,\n",
    "                                filters=512,\n",
    "                                kernel_size=1,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv5_1')\n",
    "    conv5_2 = wtf.layers.conv2d(conv5_1,\n",
    "                                filters=512,\n",
    "                                kernel_size=1,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv5_2')\n",
    "    conv5_3 = wtf.layers.conv2d(conv5_2,\n",
    "                                filters=512,\n",
    "                                kernel_size=1,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv5_3')\n",
    "    pool5 = wtf.layers.max_pooling2d(conv5_3,\n",
    "                                     pool_size=2,\n",
    "                                     strides=2,\n",
    "                                     padding='same',\n",
    "                                     name='pool5')\n",
    "    \n",
    "    conv6_1 = wtf.layers.conv2d(pool5,\n",
    "                                filters=1,\n",
    "                                kernel_size=1,\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                activation=wtf.nn.relu,\n",
    "                                kernel_initializer=wtf.variance_scaling_initializer(scale=2.0),\n",
    "                                name='conv6_1')\n",
    "        \n",
    "    dout = wtf.layers.dropout(conv6_1,\n",
    "                              rate=0.0)\n",
    "    \n",
    "    outputs = dout\n",
    "    return outputs, pool1, pool3, pool4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_fn(inputs, pool1, pool3, pool4):\n",
    "    deconv1 = wtf.layers.conv2d_transpose(inputs,\n",
    "                                          filters=pool4.get_shape().as_list()[-1],\n",
    "                                          kernel_size=4,\n",
    "                                          strides=2,\n",
    "                                          padding='same')\n",
    "    deconv1_skip = wtf.add(deconv1, pool4)\n",
    "    \n",
    "    deconv2 = wtf.layers.conv2d_transpose(deconv1_skip,\n",
    "                                          filters=pool3.get_shape().as_list()[-1],\n",
    "                                          kernel_size=4,\n",
    "                                          strides=2,\n",
    "                                          padding='same')\n",
    "    deconv2_skip = wtf.add(deconv2, pool3)\n",
    "    \n",
    "    deconv3 = wtf.layers.conv2d_transpose(deconv2_skip,\n",
    "                                          filters=1,\n",
    "                                          kernel_size=16,\n",
    "                                          strides=8,\n",
    "                                          padding='same')\n",
    "#     deconv3_skip = wtf.add(deconv3, pool1)\n",
    "    \n",
    "    outputs = deconv3\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0,t1,t2 -> t3 + LSTM\n",
    "\n",
    "def model_fn(features,\n",
    "             labels,\n",
    "             mode,\n",
    "             params):\n",
    "    with wtf.device('/device:GPU:1'):\n",
    "        inputs = features\n",
    "        print(inputs)\n",
    "        inputs_tr = wtf.transpose(inputs, [2, 3, 4, 1, 0])\n",
    "        inputs_re = wtf.reshape(inputs_tr, [192, 224, 3, -1])\n",
    "        inputs_NHWC = wtf.transpose(inputs_re, [3, 0, 1, 2])\n",
    "        print(inputs_NHWC)\n",
    "        dout, pool1, pool3, pool4 = encoder_fn(inputs_NHWC) # dout (15*N, 6, 7, 1)\n",
    "        dout_tr = wtf.transpose(dout, [1, 2, 3, 0])\n",
    "        dout_re = wtf.reshape(dout_tr, [6*7*1, 15, -1])\n",
    "        dout_NTC = wtf.transpose(dout_re, [2, 1, 0])\n",
    "        lstm_cell = rnn.LSTMCell(num_units=6*7*1,\n",
    "                                 forget_bias=1.0,\n",
    "                                 reuse=None,\n",
    "                                 name=None)\n",
    "        rnn_outputs, _ = wtf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                            inputs=dout_NTC,\n",
    "                                            dtype=wtf.float32)\n",
    "#         print(rnn_outputs) # I'm not sure what shape the rnn outputs\n",
    "        rnn_tr = wtf.transpose(rnn_outputs, [2, 1, 0])\n",
    "        rnn_re = wtf.reshape(rnn_tr, [6, 7, 1, -1])\n",
    "        rnn_NHWC = wtf.transpose(rnn_re, [3, 0, 1, 2])\n",
    "        print(rnn_NHWC)\n",
    "        deconv3 = decoder_fn(rnn_NHWC, pool1, pool3, pool4)\n",
    "        dc3_tr = wtf.transpose(deconv3, [1, 2, 3, 0])\n",
    "        dc3_re = wtf.reshape(dc3_tr, [192, 224, 1, 15, -1])\n",
    "        outputs = wtf.transpose(dc3_re, [4, 3, 0, 1, 2])\n",
    "        print(outputs)\n",
    "        \n",
    "        # Adds concatenated inputs and outputs to be displayed in TensorBoard\n",
    "        disp00 = wtf.concat([wtf.expand_dims(inputs[:, 0,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 0,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 0,:,:,2], -1),\n",
    "                             labels[:, 0], outputs[:, 0]], 1)\n",
    "        disp01 = wtf.concat([wtf.expand_dims(inputs[:, 1,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 1,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 1,:,:,2], -1),\n",
    "                             labels[:, 1], outputs[:, 1]], 1)\n",
    "        disp02 = wtf.concat([wtf.expand_dims(inputs[:, 2,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 2,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 2,:,:,2], -1),\n",
    "                             labels[:, 2], outputs[:, 2]], 1)\n",
    "        disp03 = wtf.concat([wtf.expand_dims(inputs[:, 3,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 3,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 3,:,:,2], -1),\n",
    "                             labels[:, 3], outputs[:, 3]], 1)\n",
    "        disp04 = wtf.concat([wtf.expand_dims(inputs[:, 4,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 4,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 4,:,:,2], -1),\n",
    "                             labels[:, 4], outputs[:, 4]], 1)\n",
    "        disp05 = wtf.concat([wtf.expand_dims(inputs[:, 5,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 5,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 5,:,:,2], -1),\n",
    "                             labels[:, 5], outputs[:, 5]], 1)\n",
    "        disp06 = wtf.concat([wtf.expand_dims(inputs[:, 6,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 6,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 6,:,:,2], -1),\n",
    "                             labels[:, 6], outputs[:, 6]], 1)\n",
    "        disp07 = wtf.concat([wtf.expand_dims(inputs[:, 7,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 7,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 7,:,:,2], -1),\n",
    "                             labels[:, 7], outputs[:, 7]], 1)\n",
    "        disp08 = wtf.concat([wtf.expand_dims(inputs[:, 8,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 8,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 8,:,:,2], -1),\n",
    "                             labels[:, 8], outputs[:, 8]], 1)\n",
    "        disp09 = wtf.concat([wtf.expand_dims(inputs[:, 9,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:, 9,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:, 9,:,:,2], -1),\n",
    "                             labels[:, 9], outputs[:, 9]], 1)\n",
    "        disp10 = wtf.concat([wtf.expand_dims(inputs[:,10,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:,10,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:,10,:,:,2], -1),\n",
    "                             labels[:,10], outputs[:,10]], 1)\n",
    "        disp11 = wtf.concat([wtf.expand_dims(inputs[:,11,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:,11,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:,11,:,:,2], -1),\n",
    "                             labels[:,11], outputs[:,11]], 1)\n",
    "        disp12 = wtf.concat([wtf.expand_dims(inputs[:,12,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:,12,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:,12,:,:,2], -1),\n",
    "                             labels[:,12], outputs[:,12]], 1)\n",
    "        disp13 = wtf.concat([wtf.expand_dims(inputs[:,13,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:,13,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:,13,:,:,2], -1),\n",
    "                             labels[:,13], outputs[:,13]], 1)\n",
    "        disp14 = wtf.concat([wtf.expand_dims(inputs[:,14,:,:,0], -1),\n",
    "                             wtf.expand_dims(inputs[:,14,:,:,1], -1),\n",
    "                             wtf.expand_dims(inputs[:,14,:,:,2], -1),\n",
    "                             labels[:,14], outputs[:,14]], 1)\n",
    "        disp = wtf.concat([disp00, disp01, disp02, disp03, disp04, disp05, disp06,\n",
    "                           disp07, disp08, disp09, disp10, disp11, disp12, disp13,\n",
    "                           disp14], 2)\n",
    "        \n",
    "        wtf.summary.image('concat', disp, max_outputs=200)\n",
    "    \n",
    "        predictions = {'results': outputs}\n",
    "    \n",
    "        if mode == wtf.estimator.ModeKeys.PREDICT:\n",
    "            export_outputs = {\n",
    "                'predictions': wtf.estimator.export.PredictOutput(outputs)\n",
    "            }\n",
    "            return wtf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=outputs\n",
    "            )\n",
    "    \n",
    "        # loss = wtf.losses.mean_squared_error(labels, outputs)\n",
    "#         loss = wtf.reduce_sum(wtf.abs(labels - outputs), axis=[1,2,3])\n",
    "        loss = wtf.reduce_sum(wtf.abs(labels - outputs), axis=[2,3,4])\n",
    "        loss = wtf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = wtf.train.get_global_step()\n",
    "        lr = wtf.train.exponential_decay(params.learning_rate, global_step, 10000, 0.96, staircase=True)\n",
    "        \n",
    "        wtf.summary.scalar('learning_rate', lr)\n",
    "#         wtf.summary.scalar('learning_rate', params.learning_rate)\n",
    "        \n",
    "        optimizer = wtf.train.AdamOptimizer(\n",
    "#             learning_rate=params.learning_rate,\n",
    "            learning_rate=lr,\n",
    "            beta1=params.beta1,\n",
    "            beta2=params.beta2\n",
    "        )\n",
    "    \n",
    "        update_ops = wtf.get_collection(wtf.GraphKeys.UPDATE_OPS)\n",
    "        with wtf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss, global_step=wtf.train.get_global_step())\n",
    "    \n",
    "        eval_metric_ops = {\n",
    "            \"mse_i1\": wtf.metrics.mean_squared_error(labels, inputs[:,:,:,:,0]),\n",
    "            \"mse_i2\": wtf.metrics.mean_squared_error(labels, inputs[:,:,:,:,1]),\n",
    "            \"mse_i3\": wtf.metrics.mean_squared_error(labels, inputs[:,:,:,:,2]),\n",
    "            \"mse_o\": wtf.metrics.mean_squared_error(labels, outputs)\n",
    "        }\n",
    "        \n",
    "        eval_summary_hook = wtf.train.SummarySaverHook(\n",
    "                                save_steps=1,\n",
    "                                output_dir= \"summary/train/\" + MODEL_NAME + \"/eval\",\n",
    "                                scaffold=wtf.train.Scaffold(summary_op=wtf.summary.merge_all()))\n",
    "        \n",
    "        evaluation_hooks=[]\n",
    "        evaluation_hooks.append(eval_summary_hook)\n",
    "    \n",
    "        estimator_spec = wtf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=eval_metric_ops,\n",
    "            evaluation_hooks=evaluation_hooks\n",
    "        )\n",
    "        \n",
    "        return estimator_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    \n",
    "    wtf.logging.set_verbosity(wtf.logging.INFO)\n",
    "\n",
    "    hparams = wtf.contrib.training.HParams(\n",
    "        #num_epochs=1,\n",
    "        #batch_size = 80,\n",
    "        #forget_bias=1.0,\n",
    "        learning_rate=0.0001,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999\n",
    "    )\n",
    "    \n",
    "    session_config = wtf.ConfigProto()\n",
    "    session_config.gpu_options.allow_growth = True\n",
    "    session_config.allow_soft_placement = True\n",
    "    \n",
    "    run_config = wtf.estimator.RunConfig(\n",
    "        log_step_count_steps=10,\n",
    "        save_summary_steps=10,\n",
    "        tf_random_seed=19830610,\n",
    "        model_dir=os.path.join('summary', 'train', MODEL_NAME),\n",
    "        session_config=session_config\n",
    "    )\n",
    "    \n",
    "    estimator = wtf.estimator.Estimator(\n",
    "        model_fn=model_fn, params=hparams, config=run_config)\n",
    "    \n",
    "    train_dataset = du.create_dataset_tea5poon('train', train_data)\n",
    "    train_dataset = train_dataset.shuffle(500)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "    train_dataset = train_dataset.repeat(EPOCHS)\n",
    "    \n",
    "    val_dataset = du.create_dataset_tea5poon('val', val_data)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    val_dataset = val_dataset.repeat(1)\n",
    "    \n",
    "    test_dataset = du.create_dataset_tea5poon('test', test_data)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.repeat(1)\n",
    "    \n",
    "    def train_input_fn():\n",
    "        train_iterator = train_dataset.make_one_shot_iterator()\n",
    "        features, labels = train_iterator.get_next()\n",
    "        return features, labels\n",
    "    \n",
    "    def val_input_fn():\n",
    "        val_iterator = val_dataset.make_one_shot_iterator()\n",
    "        features, labels = val_iterator.get_next()\n",
    "        return features, labels\n",
    "    \n",
    "    def test_input_fn():\n",
    "        test_iterator = test_dataset.make_one_shot_iterator()\n",
    "        features, labels = test_iterator.get_next()\n",
    "        return features, labels\n",
    "    \n",
    "    estimator.train(input_fn=train_input_fn, max_steps=None)\n",
    "#     estimator.evaluate(input_fn=val_input_fn)\n",
    "#     estimator.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    wtf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
